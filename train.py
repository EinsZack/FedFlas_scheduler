# è®­ç»ƒå…¥å£
import os

from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from torch.utils.data import get_worker_info

from scheduler_1115.envs import gymEnv, Env
from scheduler_1115.envs import GWOEnv, NewGWOEnv
import FedServer_agent
from scheduler_1115.GWO.Splitor import Splitor
from scheduler_1115.GWO.Splitor_all import Splitor_All
from utils_DQN import *
# from Splitor_RS_PPO import Splitor_RS_PPO, PPOAgent
import matplotlib.pyplot as plt
from stable_baselines3 import PPO, SAC, DQN
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback
from stable_baselines3.common.results_plotter import load_results, ts2xy


# def train_ppo_agent(ES_list, client_list, split_num_list, model_type="cifar100", total_timesteps=20000):
#     """
#     å°è£… PPO ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚
#
#     Args:
#         ES_list: Edge Server åˆ—è¡¨ã€‚
#         client_list: Client åˆ—è¡¨ã€‚
#         split_num_list: è·¯å¾„åˆ‡åˆ†æ•°é‡åˆ—è¡¨ã€‚
#         model_type: æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²ã€‚
#         total_timesteps: è®­ç»ƒçš„æ€»æ—¶é—´æ­¥æ•°ã€‚
#     """
#
#     # --- 1. ç¯å¢ƒå‡†å¤‡ ---
#     def make_ppo_env():
#         env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type)
#         # PPO æœ€å¥½ä½¿ç”¨ Monitor åŒ…è£…ï¼Œæ–¹ä¾¿ TensorBoard è®°å½•
#         return Monitor(env)
#
#         # PPO æ˜¯ On-Policy ç®—æ³•ï¼Œé€šå¸¸ä½¿ç”¨ n_envs > 1 (å¦‚æœæœ‰å¤šä¸ª CPU æ ¸) æ¥åŠ é€Ÿæ•°æ®æ”¶é›†
#
#     # è¿™é‡Œæˆ‘ä»¬ä¿æŒ n_envs=1ï¼Œä½†å¯ä»¥ä½¿ç”¨ SubprocVecEnv æ¥åŠ é€Ÿ (éœ€åœ¨ make_vec_env ä¸­è®¾ç½®)
#     vec_env = make_vec_env(make_ppo_env, n_envs=1, vec_env_cls=DummyVecEnv)
#
#     # --- 2. ç›®å½•è®¾ç½® ---
#     log_dir = "./logs/ppo_train"
#     save_dir = "./models/ppo_agent"
#     os.makedirs(log_dir, exist_ok=True)
#     os.makedirs(save_dir, exist_ok=True)
#
#     # --- 3. æ¨¡å‹å®ä¾‹åŒ– ---
#     print(f"--- PPO Agent Training ({total_timesteps} steps) ---")
#     model = PPO(
#         "MlpPolicy",
#         vec_env,
#         verbose=1,
#         learning_rate=0.0003,
#         n_steps=200,  # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
#         batch_size=64,
#         n_epochs=10,
#         gamma=0.98,
#         gae_lambda=0.94,
#         clip_range=0.2,
#         ent_coef=0.05,
#         vf_coef=0.5,
#         max_grad_norm=0.5,
#         tensorboard_log=log_dir,
#         device="auto"
#     )
#
#     # --- 4. è®¾ç½®å›è°ƒ (å¯é€‰) ---
#     checkpoint_callback = CheckpointCallback(
#         save_freq=total_timesteps // 5,  # æ¯è®­ç»ƒ 1/5 çš„æ€»æ­¥æ•°ä¿å­˜ä¸€æ¬¡
#         save_path=save_dir,
#         name_prefix="ppo_model"
#     )
#
#     # --- 5. å¼€å§‹è®­ç»ƒ ---
#     model.learn(
#         total_timesteps=total_timesteps,
#         callback=checkpoint_callback,
#         tb_log_name="PPO_run"
#     )
#
#     # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
#     final_model_path = os.path.join(save_dir, "final_ppo_model.zip")
#     model.save(final_model_path)
#     print(f"PPO è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ° {final_model_path}")
#
#     return final_model_path


def train_ppo_agent(ES_list, client_list, split_num_list, model_type="cifar100", total_timesteps=100000, bandwidth=60):
    """
    å°è£… PPO ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ·»åŠ  EvalCallback è¿›è¡Œè¯„ä¼°å’Œå¯è§†åŒ–ã€‚
    ... (Args éƒ¨åˆ†ç•¥) ...
    """

    # --- 1. ç¯å¢ƒå‡†å¤‡ ---
    # è®­ç»ƒç¯å¢ƒ (Training Environment)
    def make_ppo_env():
        env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type,bandwidth=bandwidth)
        return Monitor(env)

    vec_env = make_vec_env(make_ppo_env, n_envs=1, vec_env_cls=DummyVecEnv)

    # 2. åŒ…è£… VecEnvï¼Œå¯ç”¨çŠ¶æ€å½’ä¸€åŒ–
    vec_env = VecNormalize(
        vec_env,
        norm_obs=True,  # <--- å¯ç”¨è§‚æµ‹ï¼ˆçŠ¶æ€ï¼‰å½’ä¸€åŒ–
        norm_reward=False,  # å¥–åŠ±å½’ä¸€åŒ–é€šå¸¸æ˜¯å¯é€‰çš„ï¼Œè¿™é‡Œå…ˆä¿æŒå…³é—­
        clip_obs=10.0  # é˜²æ­¢å½’ä¸€åŒ–åçš„å€¼è¿‡å¤§
    )

    # è¯„ä¼°ç¯å¢ƒ (Evaluation Environment)
    def make_eval_env():
        env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type, bandwidth=bandwidth)
        return Monitor(env)

    # è¯„ä¼°ç¯å¢ƒé€šå¸¸ä¹Ÿä½¿ç”¨ n_envs=1
    eval_env = make_vec_env(make_eval_env, n_envs=1, vec_env_cls=DummyVecEnv)

    eval_env = VecNormalize(
        eval_env,
        norm_obs=True,
        norm_reward=False,
        clip_obs=10.0,
        # åœ¨è®­ç»ƒç»“æŸåï¼Œä½ éœ€è¦ä¿å­˜å¹¶åŠ è½½è®­ç»ƒç¯å¢ƒçš„ statsï¼Œä»¥ä¾›è¯„ä¼°ç¯å¢ƒä½¿ç”¨ã€‚
    )

    # --- 2. ç›®å½•è®¾ç½® ---
    log_dir = "./logs/ppo_train"
    save_dir = "./models/ppo_agent"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # è¯„ä¼°æ—¥å¿—ç›®å½• (ç”¨äºå­˜å‚¨è¯„ä¼°æ—¶çš„å¹³å‡å¥–åŠ±ã€é•¿åº¦ç­‰ç»Ÿè®¡ä¿¡æ¯)
    eval_log_dir = os.path.join(log_dir, "eval_results")
    os.makedirs(eval_log_dir, exist_ok=True)

    # --- 3. æ¨¡å‹å®ä¾‹åŒ– ---
    # ... (æ¨¡å‹å®ä¾‹åŒ–ä»£ç ä¿æŒä¸å˜) ...
    print(f"--- PPO Agent Training ({total_timesteps} steps) ---")
    model = PPO(
        "MlpPolicy",
        vec_env,
        verbose=1,
        learning_rate=0.0003,
        n_steps=256,  # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
        batch_size=64,
        n_epochs=10,
        gamma=0.98,
        gae_lambda=0.94,
        clip_range=0.2,
        ent_coef=0.05,
        vf_coef=0.5,
        max_grad_norm=0.5,
        tensorboard_log=log_dir,
        device="auto"
    )

    # --- 4. è®¾ç½®å›è°ƒ (å¯é€‰) ---
    # 1. ä¿å­˜æ£€æŸ¥ç‚¹çš„å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=total_timesteps // 10,
        save_path=save_dir,
        name_prefix="ppo_model"
    )

    # 2. è¯„ä¼°å›è°ƒ (EvalCallback)
    # å®ƒä¼šå®šæœŸè¯„ä¼° Agentï¼Œå¹¶å°†æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ä¿å­˜åˆ° best_model_save_path
    # eval_freq = max(total_timesteps // 10, 1000)  # è‡³å°‘ 1000 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼Œæˆ–æ€»æ­¥æ•°çš„ 1/10
    eval_freq = 10000
    best_model_save_path = os.path.join(save_dir, "best_model")
    os.makedirs(best_model_save_path, exist_ok=True)

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=best_model_save_path,
        log_path=eval_log_dir,  # è¯„ä¼°ç»“æœæ—¥å¿—è·¯å¾„
        eval_freq=eval_freq,
        deterministic=False,  # <--- ä¿æŒç­–ç•¥çš„éšæœºæ€§ï¼Œé¿å…æ”¶æ•›åˆ°æ¬¡ä¼˜çš„é›†ä¸­åˆ†é…
        render=False,
        verbose=1
    )

    # åˆå¹¶å›è°ƒåˆ—è¡¨
    callback_list = [checkpoint_callback, eval_callback]

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback_list,
        tb_log_name="PPO_run"
    )

    # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(save_dir, "final_ppo_model.zip")
    model.save(final_model_path)
    print(f"PPO è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ° {final_model_path}")

    # è®­ç»ƒå®Œæˆåï¼Œå¿…é¡»ä¿å­˜ VecNormalize çš„ç»Ÿè®¡ä¿¡æ¯ï¼
    vec_env.save(os.path.join(save_dir, "vec_normalize.pkl"))

    # è¿”å›æœ€ä½³æ¨¡å‹è·¯å¾„ï¼ˆEvalCallback ä¿å­˜çš„ï¼‰
    best_model_path = os.path.join(best_model_save_path, "best_model.zip")
    if os.path.exists(best_model_path):
        print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
        return best_model_path

    return final_model_path


def test_agent_allocation(
        model_path: str,
        algorithm_cls,
        ES_list: list,
        client_list: list,
        split_num_list: np.ndarray,
        model_type: str = "cifar100",
        test_episodes: int = 5,
        deterministic: bool = True,
        bandwidth=60
):
    """
    åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¿è¡Œå¤šä¸ªå›åˆï¼Œè¾“å‡ºæ¯ä¸ªå›åˆçš„æœ€ç»ˆ ES åˆ†é…åºåˆ—ã€‚

    Args:
        model_path: æœ€ä½³æ¨¡å‹æ–‡ä»¶ (.zip) çš„è·¯å¾„ã€‚
        algorithm_cls: è¦åŠ è½½çš„ç®—æ³•ç±» (å¦‚ PPO, DQN)ã€‚
        ES_list, client_list, split_num_list, model_type: ç¯å¢ƒå‚æ•°ã€‚
        test_episodes: è¦è¿è¡Œçš„å›åˆæ•°ã€‚
        deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆargmaxï¼‰ã€‚
    """

    # ç¡®å®š VecNormalize ç»Ÿè®¡ä¿¡æ¯çš„è·¯å¾„
    # å‡è®¾ vec_normalize.pkl ä¸æ¨¡å‹ä¿å­˜åœ¨åŒä¸€çˆ¶ç›®å½•ä¸‹
    base_dir = os.path.dirname(os.path.dirname(model_path))
    vec_stats_path = os.path.join(base_dir, "vec_normalize.pkl")

    # --- 1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒ ---
    def make_test_env():
        env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type,bandwidth=bandwidth)
        return Monitor(env)

    vec_env = make_vec_env(make_test_env, n_envs=1, vec_env_cls=DummyVecEnv)

    # --- 2. åŠ è½½ VecNormalize ç»Ÿè®¡ä¿¡æ¯ ---
    if os.path.exists(vec_stats_path):
        print(f"âœ… æ‰¾åˆ°å¹¶åŠ è½½ VecNormalize ç»Ÿè®¡ä¿¡æ¯: {vec_stats_path}")
        vec_env = VecNormalize.load(vec_stats_path, vec_env)
        # å¿…é¡»ç¦ç”¨è®­ç»ƒæ¨¡å¼ï¼Œé˜²æ­¢è¯„ä¼°æ—¶ä¿®æ”¹ç»Ÿè®¡ä¿¡æ¯
        vec_env.training = False
        vec_env.norm_reward = False
    else:
        print("âš ï¸ æœªæ‰¾åˆ° VecNormalize ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¨¡å‹å¯èƒ½å› è¾“å…¥æœªå½’ä¸€åŒ–è€Œè¡¨ç°å¼‚å¸¸ã€‚")

    # --- 3. åŠ è½½æ¨¡å‹ ---
    try:
        model = algorithm_cls.load(model_path, env=vec_env)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {os.path.basename(model_path)}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    # --- 4. è¿è¡Œè¯„ä¼° ---
    print("\n" + "=" * 50)
    print(f"ğŸš€ å¼€å§‹æµ‹è¯• {algorithm_cls.__name__} (Deterministic={deterministic}, Episodes={test_episodes})")
    print("=" * 50)

    all_allocations = []

    for episode in range(test_episodes):
        # obs, info = vec_env.reset()
        try:
            obs, info = vec_env.reset()
        except ValueError:
            # å‡è®¾åªè¿”å›äº† obs
            obs = vec_env.reset()
            info = [{}]  # ä¸ºå…¼å®¹åç»­ä»£ç ï¼Œæ·»åŠ ä¸€ä¸ªåŒ…å«ç©ºå­—å…¸çš„åˆ—è¡¨ï¼ˆå› ä¸ºæ˜¯ VecEnvï¼‰

        done = False
        step = 0

        # ç”¨äºè®°å½•æœ¬æ¬¡å›åˆçš„åˆ†é…åºåˆ—
        episode_allocations = []

        # è¿è¡Œç›´åˆ°å›åˆç»“æŸ (æ‰€æœ‰è·¯å¾„åˆ†é…å®Œæ¯•)
        while not done:
            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
            action, _ = model.predict(obs, deterministic=deterministic)

            # è®°å½•æœ¬æ¬¡åˆ†é…çš„åŠ¨ä½œ (ES ç´¢å¼•)
            # action[0] æ˜¯å› ä¸º vec_env åŒ…è£…äº†ä¸€å±‚ï¼Œå³ä½¿ n_envs=1
            episode_allocations.append(action[0])

            # obs, reward, terminated, truncated, info = vec_env.step(action)

            # try:
            #     # å°è¯•æ¥æ”¶ 5 ä¸ªå€¼ (æ–°ç‰ˆ API)
            #     obs, reward, terminated, truncated, info = vec_env.step(action)
            # except ValueError:
            # å¦‚æœæŠ¥é”™ï¼Œè¯´æ˜æ˜¯æ—§ç‰ˆ APIï¼Œåªè¿”å› 4 ä¸ªå€¼ (obs, reward, done, info)
            # æ­¤æ—¶ï¼Œterminated å’Œ truncated éƒ½åŒ…å«åœ¨æ—§çš„ done ä¸­
            obs, reward, done_old, info = vec_env.step(action)

            # åœ¨ SB3/Monitor ç¯å¢ƒä¸­ï¼Œå½“ done_old ä¸º True æ—¶ï¼Œ
            # terminated æˆ– truncated è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯ Trueã€‚
            # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ done_old æ¥å®šä¹‰å¾ªç¯è·³å‡ºæ¡ä»¶ã€‚
            terminated = done_old  # å‡è®¾æ‰€æœ‰ç»“æŸéƒ½ç®—æ˜¯ terminated
            truncated = np.zeros_like(terminated)  # å‡è®¾æ²¡æœ‰ä¸“é—¨çš„ truncated ä¿¡å·

            done = terminated or truncated
            step += 1

        # æå–æœ€ç»ˆçš„ Makespan
        final_makespan = info[0].get('makespan', 'N/A')
        allocation = info[0].get('allocation', 'N/A')
        client_time_list = info[0].get('client_time_list', 'N/A')

        print(f"\n--- Episode {episode + 1} ---")
        print(f"  æ€»æ­¥éª¤æ•° (Total Steps): {step}")
        print(f"  æœ€ç»ˆ Makespan (Final Makespan): {final_makespan:.4f}")
        print(f"  æœ€ç»ˆå„ä¸ªè®¾å¤‡æ—¶å»¶ (Device Makespans): {client_time_list}")
        print(f"  ES åˆ†é…åºåˆ— (Allocation Sequence):")
        # æ‰“å°åºåˆ—ï¼Œæ¯10ä¸ªæ¢è¡Œï¼Œæ–¹ä¾¿æŸ¥çœ‹è´Ÿè½½å‡è¡¡æƒ…å†µ

        allocation_str = " -> ".join(map(str, episode_allocations))
        print(f"    {allocation_str}")

        all_allocations.append(episode_allocations)
        print(f"  åˆ†é…çŸ©é˜µ (Allocation Matrix):")
        print(info[0].get('allocation', 'N/A'))

    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæˆã€‚")

    return all_allocations


def train_sac_agent(ES_list, client_list, split_num_list, model_type="cifar100", total_timesteps=100000):
    """
    å°è£… SAC ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚

    Args:
        ES_list, client_list, split_num_list, model_type: åŒ PPOã€‚
        total_timesteps: è®­ç»ƒçš„æ€»æ—¶é—´æ­¥æ•° (SAC é€šå¸¸éœ€è¦æ›´å¤š)ã€‚
    """

    # --- 1. ç¯å¢ƒå‡†å¤‡ ---
    def make_sac_env():
        env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type)
        return Monitor(env)

    # SAC æ˜¯ Off-Policy ç®—æ³•ï¼Œä½¿ç”¨ DummyVecEnv ä¸” n_envs=1 å³å¯
    vec_env = make_vec_env(make_sac_env, n_envs=1, vec_env_cls=DummyVecEnv)

    # --- 2. ç›®å½•è®¾ç½® ---
    log_dir = "./logs/sac_train"
    save_dir = "./models/sac_agent"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # --- 3. æ¨¡å‹å®ä¾‹åŒ– ---
    print(f"--- SAC Agent Training ({total_timesteps} steps) ---")
    model = SAC(
        "MlpPolicy",
        vec_env,
        verbose=1,
        # === SAC/Off-Policy å‚æ•° ===
        buffer_size=50000,  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        learning_starts=1000,  # æ”¶é›† 1000 æ­¥åå¼€å§‹è®­ç»ƒ
        batch_size=256,
        train_freq=(1, 'step'),  # æ¯æ”¶é›† 1 æ­¥æ•°æ®è®­ç»ƒ 1 æ¬¡
        gradient_steps=1,  # æ¯æ¬¡è®­ç»ƒæ‰§è¡Œ 1 æ¬¡æ¢¯åº¦æ›´æ–°
        ent_coef='auto',  # è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
        tau=0.005,  # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç‡
        # === é€šç”¨å‚æ•° ===
        learning_rate=0.0003,
        gamma=0.98,
        tensorboard_log=log_dir,
        device="auto"
    )

    # --- 4. è®¾ç½®å›è°ƒ (å¯é€‰) ---
    checkpoint_callback = CheckpointCallback(
        save_freq=total_timesteps // 5,
        save_path=save_dir,
        name_prefix="sac_model"
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    model.learn(
        total_timesteps=total_timesteps,
        callback=checkpoint_callback,
        tb_log_name="SAC_run"
    )

    # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(save_dir, "final_sac_model.zip")
    model.save(final_model_path)
    print(f"SAC è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ° {final_model_path}")

    return final_model_path


def train_dqn_agent(ES_list, client_list, split_num_list, model_type="cifar100", total_timesteps=100000):
    """
    å°è£… DQN ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚DQN ä¸“ä¸ºç¦»æ•£åŠ¨ä½œç©ºé—´è®¾è®¡ã€‚

    Args:
        ES_list: Edge Server åˆ—è¡¨ã€‚
        client_list: Client åˆ—è¡¨ã€‚
        split_num_list: è·¯å¾„åˆ‡åˆ†æ•°é‡åˆ—è¡¨ã€‚
        model_type: æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²ã€‚
        total_timesteps: è®­ç»ƒçš„æ€»æ—¶é—´æ­¥æ•° (DQN é€šå¸¸éœ€è¦æ›´å¤š)ã€‚
    """

    # --- 1. ç¯å¢ƒå‡†å¤‡ ---
    def make_dqn_env():
        env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type)
        return Monitor(env)

    # DQN æ˜¯ Off-Policy ç®—æ³•ï¼Œä½¿ç”¨ DummyVecEnv ä¸” n_envs=1 å³å¯
    vec_env = make_vec_env(make_dqn_env, n_envs=1, vec_env_cls=DummyVecEnv)

    # --- 2. ç›®å½•è®¾ç½® ---
    log_dir = "./logs/dqn_train"
    save_dir = "./models/dqn_agent"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # --- 3. æ¨¡å‹å®ä¾‹åŒ– ---
    print(f"--- DQN Agent Training ({total_timesteps} steps) ---")

    # DQN ç‰¹æœ‰è¶…å‚æ•°é…ç½®
    model = DQN(
        "MlpPolicy",
        vec_env,
        verbose=1,
        # === DQN æ ¸å¿ƒå‚æ•° ===
        buffer_size=50000,  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        learning_starts=1000,  # æ”¶é›† 1000 æ­¥åå¼€å§‹è®­ç»ƒ
        batch_size=128,  # æ¯æ¬¡ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        train_freq=(4, 'step'),  # æ¯æ”¶é›† 4 æ­¥æ•°æ®è®­ç»ƒ 1 æ¬¡ (å…¸å‹è®¾ç½®)
        target_update_interval=500,  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
        # === æ¢ç´¢ç‡è¡°å‡å‚æ•° ===
        exploration_fraction=0.1,  # åœ¨å‰ 10% çš„æ—¶é—´æ­¥ä¸­è¡°å‡æ¢ç´¢ç‡
        exploration_final_eps=0.05,  # æœ€ç»ˆçš„æœ€å°æ¢ç´¢ç‡

        # === é€šç”¨å‚æ•° ===
        learning_rate=0.0003,
        gamma=0.98,
        tensorboard_log=log_dir,
        device="auto"
    )

    # --- 4. è®¾ç½®å›è°ƒ (å¯é€‰) ---
    checkpoint_callback = CheckpointCallback(
        save_freq=total_timesteps // 5,
        save_path=save_dir,
        name_prefix="dqn_model"
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    model.learn(
        total_timesteps=total_timesteps,
        callback=checkpoint_callback,
        tb_log_name="DQN_run"
    )

    # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(save_dir, "final_dqn_model.zip")
    model.save(final_model_path)
    print(f"DQN è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ° {final_model_path}")

    return final_model_path

if __name__ == '__main__':
    # ä¸€äº›å‚æ•°
    num_tasks = 16

    # åˆ›å»ºå„ä¸ªè®¾å¤‡ï¼ŒåŒ…æ‹¬ESï¼ŒClientå’ŒFedServer
    # å„ä¸ªESå’ŒClient
    ES_list = []
    client_list = []
    # ================è¿™ä¸€éƒ¨åˆ†æ˜¯MNISTçš„æ•°æ®é‡è®¾å®š================
    # scene1
    # client_list.append(Env.Client(1, 42864, 328))
    # client_list.append(Env.Client(2, 35813, 328)) #
    # client_list.append(Env.Client(3, 43798, 281))
    # ES_list.append(Env.ES(11, 621409))
    # ES_list.append(Env.ES(12, 735531))
    # ES_list.append(Env.ES(13, 405849))
    # ES_list.append(Env.ES(14, 460893))
    # ES_list.append(Env.ES(15, 534774))
    # scene2
    # client_list.append(Env.Client(1, 42864, 196))
    # client_list.append(Env.Client(2, 35813, 150))
    # client_list.append(Env.Client(3, 43798, 196))
    # client_list.append(Env.Client(4, 43290, 196))
    # client_list.append(Env.Client(5, 37754, 196))
    # ES_list.append(Env.ES(6, 621409))
    # ES_list.append(Env.ES(7, 735531))
    # ES_list.append(Env.ES(8, 405849))
    # scene3
    # client_list.append(Env.Client(1, 42864, 87))
    # client_list.append(Env.Client(2, 35813, 89))
    # client_list.append(Env.Client(3, 43798, 89))
    # client_list.append(Env.Client(4, 43290, 98))
    # client_list.append(Env.Client(5, 37754, 98))
    # client_list.append(Env.Client(6, 42590, 92))
    # client_list.append(Env.Client(7, 38999, 98))
    # client_list.append(Env.Client(8, 36477, 87))
    # client_list.append(Env.Client(9, 49122, 98))
    # client_list.append(Env.Client(10, 33303, 98))  #
    # ES_list.append(Env.ES(21, 621409))
    # ES_list.append(Env.ES(22, 735531))
    # ES_list.append(Env.ES(23, 405849))
    # ES_list.append(Env.ES(24, 460893))
    # ES_list.append(Env.ES(25, 534774))

    # ================è¿™ä¸€åŠæ˜¯CIFAR10çš„æ•°æ®é‡è®¾å®š================
    # scene1
    # client_list.append(Env.Client(1, 42864, 242))
    # client_list.append(Env.Client(2, 35813, 282)) #
    # client_list.append(Env.Client(3, 43798, 256))
    # ES_list.append(Env.ES(11, 621409))
    # ES_list.append(Env.ES(12, 735531))
    # ES_list.append(Env.ES(13, 405849))
    # ES_list.append(Env.ES(14, 460893))
    # ES_list.append(Env.ES(15, 534774))
    # scene2
    # client_list.append(Env.Client(1, 42864, 160))
    # client_list.append(Env.Client(2, 35813, 187))
    # client_list.append(Env.Client(3, 43798, 180))
    # client_list.append(Env.Client(4, 43290, 138))
    # client_list.append(Env.Client(5, 37754, 114))
    # ES_list.append(Env.ES(6, 621409))
    # ES_list.append(Env.ES(7, 735531))
    # ES_list.append(Env.ES(8, 405849))
    # scene3
    # client_list.append(Env.Client(1, 42864, 75))
    # client_list.append(Env.Client(2, 35813, 74))
    # client_list.append(Env.Client(3, 43798, 55))
    # client_list.append(Env.Client(4, 43290, 79))
    # client_list.append(Env.Client(5, 37754, 92))
    # client_list.append(Env.Client(6, 42590, 77))
    # client_list.append(Env.Client(7, 38999, 85))
    # client_list.append(Env.Client(8, 36477, 78))
    # client_list.append(Env.Client(9, 49122, 69))
    # client_list.append(Env.Client(10, 33303, 93)) #
    # ES_list.append(Env.ES(21, 621409))
    # ES_list.append(Env.ES(22, 735531))
    # ES_list.append(Env.ES(23, 405849))
    # ES_list.append(Env.ES(24, 460893))
    # ES_list.append(Env.ES(25, 534774))
    # ES_list.append(Env.ES(26, 716509))
    # ES_list.append(Env.ES(27, 510927))
    # ES_list.append(Env.ES(28, 685382))
    # ES_list.append(Env.ES(29, 761315))
    # ES_list.append(Env.ES(30, 408975))
    # ================è¿™ä¸€åŠæ˜¯FMNISTçš„æ•°æ®é‡è®¾å®š================
    # scene1
    # client_list.append(Env.Client(1, 42864, 311))
    # client_list.append(Env.Client(2, 35813, 288))  #
    # client_list.append(Env.Client(3, 43798, 337))
    # ES_list.append(Env.ES(11, 621409))
    # ES_list.append(Env.ES(12, 735531))
    # ES_list.append(Env.ES(13, 405849))
    # ES_list.append(Env.ES(14, 460893))
    # ES_list.append(Env.ES(15, 534774))
    # scene2
    # client_list.append(Env.Client(1, 42864, 171))
    # client_list.append(Env.Client(2, 35813, 177))
    # client_list.append(Env.Client(3, 43798, 208))
    # client_list.append(Env.Client(4, 43290, 208))
    # client_list.append(Env.Client(5, 37754, 171))
    # ES_list.append(Env.ES(6, 621409))
    # ES_list.append(Env.ES(7, 735531))
    # ES_list.append(Env.ES(8, 405849))
    # scene3
    # client_list.append(Env.Client(1, 42864, 84))
    # client_list.append(Env.Client(2, 35813, 84))
    # client_list.append(Env.Client(3, 43798, 104))
    # client_list.append(Env.Client(4, 43290, 98))
    # client_list.append(Env.Client(5, 37754, 104))
    # client_list.append(Env.Client(6, 42590, 84))
    # client_list.append(Env.Client(7, 38999, 84))
    # client_list.append(Env.Client(8, 36477, 104))
    # client_list.append(Env.Client(9, 49122, 84))
    # client_list.append(Env.Client(10, 33303, 104))  #
    # ES_list.append(Env.ES(21, 621409))
    # ES_list.append(Env.ES(22, 735531))
    # ES_list.append(Env.ES(23, 405849))
    # ES_list.append(Env.ES(24, 460893))
    # ES_list.append(Env.ES(25, 534774))
    # ================è¿™ä¸€åŠæ˜¯CIFAR100çš„æ•°æ®é‡è®¾å®š================
    # scene1
    client_list.append(Env.Client(1, 42864, 243))
    client_list.append(Env.Client(2, 35813, 264)) #
    client_list.append(Env.Client(3, 43798, 273))
    ES_list.append(Env.ES(11, 621409))
    ES_list.append(Env.ES(12, 735531))
    ES_list.append(Env.ES(13, 405849))
    ES_list.append(Env.ES(14, 460893))
    ES_list.append(Env.ES(15, 534774))
    # scene2
    # client_list.append(Env.Client(1, 42864, 160))
    # client_list.append(Env.Client(2, 35813, 161)) #
    # client_list.append(Env.Client(3, 43798, 140))
    # client_list.append(Env.Client(4, 43290, 155))
    # client_list.append(Env.Client(5, 37754, 165))
    # ES_list.append(Env.ES(6, 621409))
    # ES_list.append(Env.ES(7, 735531))
    # ES_list.append(Env.ES(8, 405849))
    # scene3
    # client_list.append(Env.Client(1, 42864, 74))
    # client_list.append(Env.Client(2, 35813, 76))
    # client_list.append(Env.Client(3, 43798, 78))
    # client_list.append(Env.Client(4, 43290, 78))
    # client_list.append(Env.Client(5, 37754, 80))
    # client_list.append(Env.Client(6, 42590, 71))
    # client_list.append(Env.Client(7, 38999, 79))
    # client_list.append(Env.Client(8, 36477, 82))
    # client_list.append(Env.Client(9, 49122, 82))
    # client_list.append(Env.Client(10, 33303, 76)) #
    # ES_list.append(Env.ES(21, 621409))
    # ES_list.append(Env.ES(22, 735531))
    # ES_list.append(Env.ES(23, 405849))
    # ES_list.append(Env.ES(24, 460893))
    # ES_list.append(Env.ES(25, 534774))
    # ES_list.append(Env.ES(26, 716509))
    # ES_list.append(Env.ES(27, 510927))
    # ES_list.append(Env.ES(28, 685382))
    # ES_list.append(Env.ES(29, 761315))
    # ES_list.append(Env.ES(30, 408975))

    model_type = "cifar100"
    bandwidth = 30


    # åˆ›å»ºEnvï¼Œå°†è®¾å¤‡ä¿¡æ¯ä¼ å…¥ç»™Env
    # gwoenv = GWOEnv.TaskAssignmentEnv(ES_list, client_list, model_type, bandwidth)  # ä¸“é—¨ç”¨äºGWOçš„Envï¼Œåé¢å†æ”¹
    gwoenv = NewGWOEnv.TaskAssignmentEnv(ES_list, client_list,None, model_type, bandwidth)  # ä¸“é—¨ç”¨äºGWOçš„Envï¼Œåé¢å†æ”¹

    # åˆ›å»ºSplitorï¼Œä½œä¸ºç¬¬ä¸€é˜¶æ®µå†³å®šè·¯å¾„åˆ‡åˆ†æ•°é‡
    # splitor = Splitor(ES_list, client_list, gwoenv)
    # split_num_list, init_dist = splitor.get_split_numlist()
    # split_num_list = split_num_list.astype(int)

    # splitor_all
    splitor_all = Splitor_All(ES_list, client_list, gwoenv)
    split_num_list, init_dist = splitor_all.get_split_numlist()
    split_num_list = split_num_list.astype(int)


    # è®­ç»ƒPPOæ™ºèƒ½ä½“
    final_ppo_model_path = train_ppo_agent(
        ES_list,
        client_list,
        split_num_list,
        model_type=model_type,
        total_timesteps=150000,
        bandwidth=bandwidth
    )

    PPO_BASE_DIR = "./models/ppo_agent"
    BEST_PPO_MODEL_PATH = os.path.join(PPO_BASE_DIR, "best_model", "best_model.zip")


    if os.path.exists(BEST_PPO_MODEL_PATH):
        # --- æµ‹è¯•ç¡®å®šæ€§æ¨¡å¼ (æ£€æŸ¥æ˜¯å¦é›†ä¸­åˆ†é…) ---
        print("\n" + "#" * 60)
        print("### æ¨¡å¼ä¸€ï¼šDeterministic=True (é›†ä¸­åˆ†é…æ£€æŸ¥) ###")
        print("#" * 60)
        test_agent_allocation(
            model_path=BEST_PPO_MODEL_PATH,
            algorithm_cls=PPO,
            ES_list=ES_list,
            client_list=client_list,
            split_num_list=split_num_list,
            model_type=model_type,
            test_episodes=5,
            deterministic=True,  # æ£€æŸ¥åªé€‰ä¸€ä¸ªESçš„é—®é¢˜
            bandwidth=bandwidth
        )

        # --- æµ‹è¯•éšæœºæ¨¡å¼ (æ£€æŸ¥è´Ÿè½½å‡è¡¡) ---
        print("\n" + "#" * 60)
        print("### æ¨¡å¼äºŒï¼šDeterministic=False (å‡è¡¡åˆ†é…æ£€æŸ¥) ###")
        print("#" * 60)
        test_agent_allocation(
            model_path=BEST_PPO_MODEL_PATH,
            algorithm_cls=PPO,
            ES_list=ES_list,
            client_list=client_list,
            split_num_list=split_num_list,
            model_type=model_type,
            test_episodes=5,
            deterministic=False,  # æ£€æŸ¥æ˜¯å¦èƒ½åˆ†æ•£åˆ†é…
            bandwidth = bandwidth
        )
    else:
        print(f"âŒ æœ€ä½³æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {BEST_PPO_MODEL_PATH}ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")

    print("å¯¹æ¯”åˆå§‹GWOçš„åˆ†é…æ–¹æ¡ˆï¼š")
    print(init_dist)
    env = gymEnv.GymPPOEnv(ES_list, client_list, split_num_list, model_type=model_type, bandwidth=bandwidth)
    init_time, client_time_list = env.calculate_makespan_for_allocation(init_dist)
    print(f"åˆå§‹åˆ†é…æ–¹æ¡ˆçš„Makespan: {init_time}")
    print(client_time_list)


    # è®­ç»ƒ SAC (ä¾‹å¦‚ 100,000 æ­¥)
    # sac_path = train_sac_agent(
    #     ES_list,
    #     client_list,
    #     split_num_list,
    #     model_type="cifar100",
    #     total_timesteps=100000
    # )

    # dqn_path = train_dqn_agent(
    #     ES_list,
    #     client_list,
    #     split_num_list,
    #     model_type="cifar100",
    #     total_timesteps=100000
    # )
